import yaml
import sys
from pathlib import Path
import steps
import tskit

configfile: "config.yaml"
shell.prefix(config["prefix"])
data_dir = Path(config["data_dir"])

def get_resource(rule_name, resource_type):
    if rule_name in config["resources"] and resource_type in config["resources"][rule_name]:
        return config["resources"][rule_name][resource_type]
    else:
        return config["resources"]["default"][resource_type]

rule all:
    input:
        expand(
            data_dir / "{model_name}-{contig}-lm{length_multiplier}"
            / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-ancestors.csv",
            model_name=config["models"],
            contig=config["contigs"],
            length_multiplier=config["length_multipliers"],
            seed=config["seeds"],
        ),
        expand(
            data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                    / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-{version}-ancestors.zarr",
            model_name=config["models"],
            contig=config["contigs"],
            length_multiplier=config["length_multipliers"],
            seed=config["seeds"],
            version=config["versions"],
        )

rule simulate:
    output:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-raw.trees"
    threads: get_resource("simulate", "threads")
    resources:
        mem_mb=get_resource("simulate", "mem_mb"),
        time_min=get_resource("simulate", "time_min")
    run:
        ts = steps.simulate(
            wildcards.model_name,
            wildcards.contig,
            config["samples"][0],
            float(wildcards.length_multiplier),
            int(wildcards.seed)
        )
        ts.dump(output[0])

rule prune_simulated_ts:
    input:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-raw.trees"
    output:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-pruned.trees"
    threads: get_resource("prune_simulated_ts", "threads")
    resources:
        mem_mb=get_resource("prune_simulated_ts", "mem_mb"),
        time_min=get_resource("prune_simulated_ts", "time_min")
    run:
        ts = tskit.load(input[0])
        ts = steps.prune_simulated_ts(ts)
        ts.dump(output[0])

rule filter_sites:
    input:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-pruned.trees"
    output:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-pruned-filtered.trees"
    threads: get_resource("filter_sites", "threads")
    resources:
        mem_mb=get_resource("filter_sites", "mem_mb"),
        time_min=get_resource("filter_sites", "time_min")
    run:
        ts = tskit.load(input[0])
        ts = steps.remove_singletons(ts)
        ts.dump(output[0])

rule generate_ancestors:
    input:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-pruned-filtered.trees"
    output:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-{version}-ancestors.zarr"
    threads: get_resource("generate_ancestors", "threads")
    resources:
        mem_mb=get_resource("generate_ancestors", "mem_mb"),
        time_min=get_resource("generate_ancestors", "time_min")
    shell:
        """
        source {config[env_dir]}/bin/activate
        python generate_ancestors.py \
            {input[0]} \
            {output[0]} \
            --version {wildcards.version} \
            --threads {threads} \
            --data-dir {config[data_dir]}
        """

checkpoint prepare_ancestor_df:
    input:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
            / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-pruned-filtered.trees",
        expand(
            data_dir / "{model_name}-{contig}-lm{length_multiplier}"
            / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-{version}-ancestors.zarr",
            model_name=config["models"],
            contig=config["contigs"],
            length_multiplier=config["length_multipliers"],
            seed=r"\d+", 
            version=config["versions"],
        )
    output:
        chunks_dir=directory(
            data_dir / "{model_name}-{contig}-lm{length_multiplier}" / "ancestor-chunks-s{seed}"
        ),
        done_file=data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                 / "ancestor-chunks-s{seed}-created.done"
    threads: get_resource("prepare_ancestor_df", "threads")
    resources:
        mem_mb=get_resource("prepare_ancestor_df", "mem_mb"),
        time_min=get_resource("prepare_ancestor_df", "time_min")
    run:
        sys.path.append(config['tsinfer_dir'])
        import tsinfer

        ts = tskit.load(input[0])
        chunk_size = config["ancestor_chunk_size"]
        output_dir = Path(output.chunks_dir)
        done_path = Path(output.done_file)
        output_dir.mkdir(parents=True, exist_ok=True)

        anc_data_list = []
        for path in input[1:]:
            anc_data = tsinfer.formats.AncestorData.load(path)
            anc_data_list.append(anc_data)

        steps.prepare_ancestor_df(
            ancestors_list=anc_data_list,
            ts_path=input[0],
            output_dir=output_dir,
            chunk_size=chunk_size,
            done_path=done_path
        )

def chunk_input(wildcards):
    checkpoint_output = checkpoints.prepare_ancestor_df.get(
        model_name=wildcards.model_name,
        contig=wildcards.contig,
        length_multiplier=wildcards.length_multiplier,
        seed=wildcards.seed,
    ).output[0]
    return checkpoint_output / f"chunk_{wildcards.chunk_id}_unprocessed.csv"

rule process_ancestor_chunk:
    input:
        chunk=chunk_input,
        done_file = data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                 / "ancestor-chunks-s{seed}-created.done",
        ts_path = data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-pruned-filtered.trees",
        anc_data_paths = data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-{version}-ancestors.zarr"
    output:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                / "ancestor-chunks-s{seed}"
                 / "chunk_{chunk_id}_processed.csv"
    threads: get_resource("process_ancestor_chunk", "threads")
    resources:
        mem_mb=get_resource("process_ancestor_chunk", "mem_mb"),
        time_min=get_resource("process_ancestor_chunk", "time_min")
    run:
        sys.path.append(config['tsinfer_dir'])
        import tsinfer

        df = pd.read_csv(input.chunk)
        ts = tskit.load(input.ts_path)
        versions = config["versions"]
        assert len(input.anc_data_paths == len(versions))
        
        anc_data_dict = {}
        for path, version in zip(input.anc_data_paths, versions):
            anc_data = tsinfer.formats.AncestorData.load(path)
            anc_data_dict[version] = anc_data

        steps.process_ancestor_chunk(
            df=df,
            ts=ts,
            anc_data_dict=anc_data_dict,
            output_path=output[0],
        )

def chunk_ids(wildcards):
    checkpoint_output = checkpoints.prepare_ancestor_df.get(
        model_name=wildcards.model_name,
        contig=wildcards.contig,
        length_multiplier=wildcards.length_multiplier,
        seed=wildcards.seed,
    ).output[0]
    chunk_dir = Path(checkpoint_output)
    return [f.stem.split("_")[1] for f in chunk_dir.glob("chunk_*_processed.csv")]

rule mark_chunks_processed:
    input:
        lambda wildcards: [
            f"data/{wildcards.model_name}-{wildcards.contig}-lm{wildcards.length_multiplier}/ancestor-chunks-s{wildcards.seed}/chunk_{i}_processed.csv"
            for i in chunk_ids(wildcards)
        ],
        data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                 / "ancestor-chunks-s{seed}-created.done"
    output:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                 / "ancestor-chunks-s{seed}-all-processed.done"
    run:
        Path(output[0]).touch()

        
def aggregate_chunk_paths(wildcards):
    checkpoint_output = checkpoints.prepare_ancestor_df.get(
        model_name=wildcards.model_name,
        contig=wildcards.contig,
        length_multiplier=wildcards.length_multiplier,
        seed=wildcards.seed,
    ).output[0]
    chunk_dir = Path(checkpoint_output)
    return sorted(str(chunk) for chunk in chunk_dir.glob("chunk_*_processed.csv"))

rule aggregate_ancestor_chunks:
    input:
        aggregate_chunk_paths,
        data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                 / "ancestor-chunks-s{seed}-all-processed.done"
    output:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}"
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-ancestors.csv"
    threads: get_resource("aggregate_ancestor_chunks", "threads")
    resources:
        mem_mb=get_resource("aggregate_ancestor_chunks", "mem_mb"),
        time_min=get_resource("aggregate_ancestor_chunks", "time_min")
    run:
        import pandas as pd
        chunk_files = input
        dfs = [pd.read_csv(f) for f in chunk_files]
        df = pd.concat(dfs, ignore_index=True)
        df.to_csv(output[0], index=False)