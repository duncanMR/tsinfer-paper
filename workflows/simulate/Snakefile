import yaml
import sys
from pathlib import Path
import steps
import json
import tskit
import pandas as pd
import numpy as np
import shutil
import warnings
import contextlib
warnings.filterwarnings("ignore", category=FutureWarning, message=".*LMDBStore is deprecated.*")

configfile: "config.yaml"
shell.prefix(config["prefix"])
data_dir = Path(config["data_dir"])
progress_dir = Path(config["progress_dir"])
sys.path.append(config['tsinfer_dir'])
import tsinfer

def get_resource(rule_name, resource_type):
    if rule_name in config["resources"] and resource_type in config["resources"][rule_name]:
        return config["resources"][rule_name][resource_type]
    else:
        return config["resources"]["default"][resource_type]

rule all:
    input:
        expand(
            data_dir / "{model_name}-{contig}-lm{length_multiplier}"
            / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-ancestors.csv",
            model_name=config["models"],
            contig=config["contigs"],
            length_multiplier=config["length_multipliers"],
            seed=config["seeds"],
        ),
        expand(
            data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                    / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-{version}-ancestors.zarr",
            model_name=config["models"],
            contig=config["contigs"],
            length_multiplier=config["length_multipliers"],
            seed=config["seeds"],
            version=config["versions"],
        ),

rule simulate:
    output:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}"
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-raw.trees"
    threads: get_resource("intensive", "threads")
    resources:
        mem_mb=get_resource("intensive", "mem_mb"),
        time_min=get_resource("intensive", "time_min")
    run:
        ts = steps.simulate(
            wildcards.model_name,
            wildcards.contig,
            config["samples"][0],
            float(wildcards.length_multiplier),
            int(wildcards.seed)
        )
        ts.dump(output[0])

rule prune_simulated_ts:
    input:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}" 
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-raw.trees"
    output:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}"
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-pruned.trees"
    threads: get_resource("intensive", "threads")
    resources:
        mem_mb=get_resource("intensive", "mem_mb"),
        time_min=get_resource("intensive", "time_min")
    run:
        ts = tskit.load(input[0])
        ts = steps.prune_simulated_ts(ts)
        ts.dump(output[0])

rule filter_sites:
    input:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}"
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-pruned.trees"
    output:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}"
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-pruned-filtered.trees"
    threads: get_resource("intensive", "threads")
    resources:
        mem_mb=get_resource("intensive", "mem_mb"),
        time_min=get_resource("intensive", "time_min")
    run:
        ts = tskit.load(input[0])
        ts = steps.remove_singletons(ts)
        ts.dump(output[0])

rule generate_ancestors:
    input:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}"
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-pruned-filtered.trees"
    output:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}"
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-{version}-ancestors.zarr"
    log:
        progress_dir / "{model_name}-{contig}-lm{length_multiplier}" \
        / "generate_ancestors-s{seed}-{version}.log"
    threads: get_resource("generate_ancestors", "threads")
    resources:
        mem_mb=get_resource("generate_ancestors", "mem_mb"),
        time_min=get_resource("generate_ancestors", "time_min")
    shell:
        """
        source {config[env_dir]}/bin/activate
        python generate_ancestors.py \
            {input[0]} \
            {output[0]} \
            {log[0]} \
            --version {wildcards.version} \
            --threads {threads} \
            --data-dir {config[data_dir]}
        """

checkpoint build_ancestor_chunks:
    input:
        ts = data_dir / "{model_name}-{contig}-lm{length_multiplier}"
            / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-pruned-filtered.trees",
        paths = expand(
            data_dir / "{model_name}-{contig}-lm{length_multiplier}"
                / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-{version}-ancestors.zarr",
            model_name=config["models"],
            contig=config["contigs"],
            length_multiplier=config["length_multipliers"],
            seed=config["seeds"],
            version=config["versions"],
        )
    output:
        metadata_json=data_dir / "{model_name}-{contig}-lm{length_multiplier}"
                      / "unprocessed-ancestor-chunks-s{seed}-metadata.json"
    threads: get_resource("intensive", "threads")
    resources:
        mem_mb=get_resource("intensive", "mem_mb"),
        time_min=get_resource("intensive", "time_min")
    run:
        anc_data_list = [
            tsinfer.formats.AncestorData.load(path)
            for path in input.paths
        ]
        ts = tskit.load(input.ts)
        metadata_path = Path(output.metadata_json)
        output_dir = metadata_path.parent / f"unprocessed-ancestor-chunks-s{wildcards.seed}"
        output_dir.mkdir(parents=True, exist_ok=True)
        chunk_size = config["ancestor_chunk_size"]
        steps.build_ancestor_chunks(
            anc_data_list=anc_data_list,
            ts=ts,
            metadata_path=metadata_path,
            output_dir=output_dir,
            chunk_size=chunk_size,
        )

def chunk_ids(wildcards):
    cp = checkpoints.build_ancestor_chunks.get(
        model_name=wildcards.model_name,
        contig=wildcards.contig,
        length_multiplier=wildcards.length_multiplier,
        seed=wildcards.seed,
    )
    with open(cp.output.metadata_json) as f:
        metadata = json.load(f)
    return list(range(metadata["num_chunks"]))


def chunk_input(wildcards):
    return str(
        data_dir
        / f"{wildcards.model_name}-{wildcards.contig}-lm{wildcards.length_multiplier}"
        / f"unprocessed-ancestor-chunks-s{wildcards.seed}"
        / f"unprocessed-chunk-{wildcards.chunk_id}.csv"
    )

rule process_ancestor_chunk:
    input:
        chunk = chunk_input,
        ts_path = data_dir / "{model_name}-{contig}-lm{length_multiplier}"
                             / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-pruned-filtered.trees",
        anc_data_paths = expand(
            data_dir / "{model_name}-{contig}-lm{length_multiplier}"
                      / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-{version}-ancestors.zarr",
            version=config["versions"],
            model_name="{model_name}",
            contig="{contig}",
            length_multiplier="{length_multiplier}",
            seed="{seed}"
        )
    output:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}" \
        / "processed-ancestor-chunks-s{seed}" \
        / "processed-chunk-{chunk_id}.csv"
    log:
        progress_dir / "{model_name}-{contig}-lm{length_multiplier}" \
        / "processed-ancestor-chunks-s{seed}" \
        / "process-chunk-{chunk_id}.log"
    threads: get_resource("process_ancestor_chunk", "threads")
    resources:
        mem_mb=get_resource("process_ancestor_chunk", "mem_mb")
    run:
        with open(log[0], "w") as log_file:
            with contextlib.redirect_stdout(log_file), contextlib.redirect_stderr(log_file):
                print(f"[INFO] Loading chunk {wildcards.chunk_id}", flush=True)
                chunk_path = Path(input.chunk)
                df = pd.read_csv(chunk_path)
                assert len(df) > 0
                print(f"[INFO] Loading tree sequence", flush=True)
                ts = tskit.load(input.ts_path)
                sites_position = np.append(ts.sites_position, ts.sequence_length)
                versions = config["versions"]
                print(f"[INFO] Importing AncestorData", flush=True)
                anc_data_dict = {
                    version: tsinfer.formats.AncestorData.load(path)
                    for path, version in zip(input.anc_data_paths, versions)
                }
                print(f"[INFO] Starting DF generation", flush=True)
                steps.process_ancestor_chunk(
                    df=df,
                    ts=ts,
                    sites_position=sites_position,
                    anc_data_dict=anc_data_dict,
                    output_path=output[0]
                )

rule process_all_chunks:
    input:
        lambda wildcards: expand(
            data_dir / "{model_name}-{contig}-lm{length_multiplier}"
                      / "processed-ancestor-chunks-s{seed}"
                      / "processed-chunk-{chunk_id}.csv",
            model_name=[wildcards.model_name],
            contig=[wildcards.contig],
            length_multiplier=[wildcards.length_multiplier],
            seed=[wildcards.seed],
            chunk_id=chunk_ids(wildcards)
        )
    output:
        temp(data_dir / "{model_name}-{contig}-lm{length_multiplier}" / "finished-ancestor-chunks-s{seed}.done")
    run:
        Path(output[0]).touch()

def aggregate_chunk_paths(wildcards):
    cp = checkpoints.build_ancestor_chunks.get(
        model_name=wildcards.model_name,
        contig=wildcards.contig,
        length_multiplier=wildcards.length_multiplier,
        seed=wildcards.seed,
    )
    base = Path(cp.output.metadata_json).parent
    processed_dir = base / f"processed-ancestor-chunks-s{wildcards.seed}"
    return sorted(str(p) for p in processed_dir.glob("processed-chunk-*.csv"))

rule aggregate_ancestor_chunks:
    input:
        done = data_dir / "{model_name}-{contig}-lm{length_multiplier}"
                          / "finished-ancestor-chunks-s{seed}.done",
        chunks = aggregate_chunk_paths
    output:
        data_dir / "{model_name}-{contig}-lm{length_multiplier}"
                 / "{model_name}-{contig}-lm{length_multiplier}-s{seed}-ancestors.csv"
    threads: get_resource("aggregate_ancestor_chunks", "threads")
    resources:
        mem_mb   = get_resource("aggregate_ancestor_chunks", "mem_mb"),
        time_min = get_resource("aggregate_ancestor_chunks", "time_min")
    run:
        import pandas as pd
        dfs = [pd.read_csv(p) for p in input.chunks]
        pd.concat(dfs, ignore_index=True).to_csv(output[0], index=False)

